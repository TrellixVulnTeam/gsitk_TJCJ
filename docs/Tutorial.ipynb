{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This short tutorial will teach you how to use the _gsitk_ library for Sentiment Analysis.\n",
    "\n",
    "Concretely, it covers:\n",
    "\n",
    "- Managing datasets with the [`DatasetManager`](https://gsi-upm.github.io/gsitk/datasets/#dataset-manager-interface) utility\n",
    "- Preprocessing textual data (although included datasets are already preprocessed)\n",
    "- Extract features with models that are implemented in _gsitk_\n",
    "- Persist to disk extracted features, and load them from disk\n",
    "- Prepare an evaluation using the [`Evaluation`](https://gsi-upm.github.io/gsitk/evaluation/) interface\n",
    "\n",
    "This tutorial has been generated using a [jupyter notebook](https://jupyter.org/) that you may download and run locally, from [here](https://github.com/gsi-upm/gsitk/blob/master/docs/Tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For running this tutorial, you need to have _gsitk_ installed. You can install using pip:\n",
    "```\n",
    "pip install gsitk\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you need to set a default data path. This is where _gsitk_ will save all the datasets.\n",
    "You can do so by setting an environment variable. If you do not specify a `$DATA_PATH`, the default path is `/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DATA_PATH=/tmp\n"
     ]
    }
   ],
   "source": [
    "%env DATA_PATH=/tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you need to download some NLTK resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/oaraque/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/oaraque/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /home/oaraque/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('opinion_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_gsitk_ largely simplifies dataset management, as it has several commonly used datasets for Sentiment Analysis.\n",
    "You can view the available datasets [here](https://gsi-upm.github.io/gsitk/datasets/#available-datasets).\n",
    "For this tutorial, we will download two datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsitk.datasets.datasets import DatasetManager\n",
    "\n",
    "dm = DatasetManager()\n",
    "data = dm.prepare_datasets(['vader', 'imdb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`prepare_datasets` downloads the data form their original sources, preprocesses the text and labels, and saves all to disk (in `$DATA_PATH`).\n",
    "In this way, next time you call `prepare_datasets`, it will run quickly.\n",
    "\n",
    "The `data` variable has now a python `dict` that contains two keys: _vader_ and _imdb_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of data: <class 'dict'>\n",
      "datasets prepared dict_keys(['vader', 'imdb'])\n"
     ]
    }
   ],
   "source": [
    "print('type of data:', type(data))\n",
    "print('datasets prepared',  data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are saved in a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[somehow, i, was, blessed, with, some, really,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[yay, ., another, good, phone, interview, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[we, were, number, deep, last, night, amp, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[lmao, allcaps, ,, amazing, allcaps, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>[two, words, that, should, die, this, year, :,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                                               text\n",
       "0         1  [somehow, i, was, blessed, with, some, really,...\n",
       "1         1       [yay, ., another, good, phone, interview, .]\n",
       "2         1  [we, were, number, deep, last, night, amp, the...\n",
       "3         1            [lmao, allcaps, ,, amazing, allcaps, !]\n",
       "4        -1  [two, words, that, should, die, this, year, :,..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['vader'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    2901\n",
       "-1    1299\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['vader']['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All available datasets in _gsitk_ can be seen [here](https://gsi-upm.github.io/gsitk/datasets/#available-datasets).\n",
    "This tool eases the replicability of sentiment analysis methods, offering a common ground for researchers to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_gsitk_ includes several functionalities for preprocessing text.\n",
    "Although all the included datasets are loaded through _gsitk_ already processed, users may want to preprocess their own datasets.\n",
    "With the funcionalities presented below, they can do so.\n",
    "\n",
    "_gsitk_ includes three types of preprocessers:\n",
    "\n",
    "- Simple: the simple and more efficient pre-processor.\n",
    "- Pre-process Twitter: a processor indicated for parsing Twitter text.\n",
    "- Normalize: An all-purpose pre-processor. \n",
    "\n",
    "For more information, please check the [documentation](https://gsi-upm.github.io/gsitk/preprocessing/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most direct way to preprocess is as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple ['the', 'earth', 'is', 'not', 'flat', ',', 'but', 'almost', '.', 'please', ',', 'believe', 'me', '!']\n",
      "twitter <user> can i have a selfie? <hastag> thanks\n",
      "normalize ['the', 'earth', 'is', 'not', 'flat', ',', 'but', 'almost', '.', 'please', ',', 'believe', 'me', '!']\n"
     ]
    }
   ],
   "source": [
    "from gsitk.preprocess import simple, pprocess_twitter, normalize\n",
    "\n",
    "text = \"The earth is not flat, but almost. Please, believe me!\"\n",
    "twitter_text = \"@POTUS can I have a selfie? #thanks\"\n",
    "\n",
    "print('simple', simple.preprocess(text))\n",
    "print('twitter', pprocess_twitter.preprocess(twitter_text))\n",
    "print('normalize', normalize.preprocess(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All preprocessing utilities implement the `preprocess` method, which can be useful for integrating these methods into your work pipeline. Nevertheless, _gsitk_ offers the `Preprocessor` interface to facilitate the use of preprocessers into its philosophy; as well as to include preprocessing into scikit-learn [Pipelines](https://scikit-learn.org/stable/modules/compose.html). A simple example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<user> can i have a selfie? <hastag> thanks',\n",
       "       \"if only bradley's arm was longer. best photo ever. <hastag> oscars\"],\n",
       "      dtype='<U66')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gsitk.preprocess import pprocess_twitter, Preprocessor\n",
    "\n",
    "texts = [\n",
    "    \"@POTUS can I have a selfie? #thanks\",\n",
    "    \"If only Bradley's arm was longer. Best photo ever. #oscars\"\n",
    "]\n",
    "Preprocessor(pprocess_twitter).transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, the `Preprocessor` utility has full compatibility with scikit-learn's Pipelines. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this cat is crazy , he is not on the mat !',\n",
       " 'will no one rid me of this turbulent priest ?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from gsitk.preprocess import normalize, Preprocessor, JoinTransformer\n",
    "\n",
    "texts = [\n",
    "    \"This cat is crazy, he is not on the mat!\",\n",
    "    \"Will no one rid me of this turbulent priest?\"\n",
    "]\n",
    "\n",
    "preprocessing_pipe = Pipeline([\n",
    "    ('twitter', Preprocessor(normalize)),\n",
    "    ('join', JoinTransformer())\n",
    "])\n",
    "\n",
    "preprocessing_pipe.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords is a common task in NLP. _gsitk_ includes a functionality (`StopWordsRemover`) that performs this task, using NLTK's stopword lists.\n",
    "As before, `StopWordsRemover` is compatible with scikit-learn's Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat crazy , mat !', 'one rid turbulent priest ?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gsitk.preprocess.stopwords import StopWordsRemover\n",
    "\n",
    "texts = [\n",
    "    \"this cat is crazy , he is not on the mat !\",\n",
    "    \"will no one rid me of this turbulent priest ?\"\n",
    "]\n",
    "\n",
    "StopWordsRemover().fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it uses the NLTK stop word collections, several languages can be parsed, as in this Spanish example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clavel blanco rosa roja , majestad escoja',\n",
       " 'diez cañones banda viento popa toda vela']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gsitk.preprocess.stopwords import StopWordsRemover\n",
    "\n",
    "texts = [\n",
    "    \"entre el clavel blanco y la rosa roja , su majestad escoja\",\n",
    "    \"con diez cañones por banda viento en popa a toda vela\",\n",
    "]\n",
    "\n",
    "StopWordsRemover(language='spanish').fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_gsitk_ has several useful feature extractors. It includes the implementation of some models proposed in research works, which aids in replicability and comparison tasks.\n",
    "These techniques have been recently published in peer-reviewed publications, and are oriented to foster research.\n",
    "We show an example of the use an embedding model, extracting word2vec features ([paper here](https://doi.org/10.1016/j.eswa.2017.02.002)) for Sentiment Analysis, and the SIMilarity-based sentiment projectiON (SIMON) model ([paper here](https://doi.org/10.1016/j.knosys.2018.12.005))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding model (Word2VecFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs an aggregation of the individual word vectors, computing an unified representation that can be used directly by a classfical machine learning classifier.\n",
    "It uses a pre-trained word embedding model to extract a vector for each word, and then applies a pooling function to all words, obtaining document-level representation. By default, the pooling function is the average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, import a word embedding model. Using [gensim](https://radimrehurek.com/gensim/) makes this step easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "embedding_model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsitk.features.word2vec import Word2VecFeatures\n",
    "\n",
    "w2v_transformer = Word2VecFeatures(model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2236732 ,  0.25583891, -0.487614  , -0.301236  ,  0.86721801,\n",
       "         0.08720819, -0.48012703,  0.03669201,  0.099744  ,  0.0686424 ,\n",
       "         0.05496354,  0.44522   , -0.11925981,  0.0202894 ,  0.58847399,\n",
       "         0.292354  ,  0.25116399,  0.470316  , -0.15011139, -0.544134  ,\n",
       "        -0.553544  ,  0.51258399,  0.36928921,  0.41198533,  0.81134399,\n",
       "        -1.76617999, -0.923298  ,  0.54818199,  0.5961172 , -0.42874679,\n",
       "         3.04592001,  0.29102398, -0.22677599,  0.087812  , -0.0293142 ,\n",
       "        -0.072712  ,  0.1456914 ,  0.24413   ,  0.05948399, -0.600286  ,\n",
       "        -0.1608764 ,  0.012316  , -0.39915799,  0.3701636 ,  0.3432422 ,\n",
       "        -0.1038574 , -0.074668  , -0.441143  ,  0.2592026 ,  0.569796  ],\n",
       "       [ 0.28759499,  0.22734876, -0.42304934, -0.26260117,  0.64443667,\n",
       "         0.12306683, -0.13500085,  0.05503167, -0.14429333,  0.1364975 ,\n",
       "         0.011965  ,  0.34951501, -0.09140483,  0.01189917,  0.49902666,\n",
       "         0.3094525 ,  0.022184  ,  0.38554393, -0.05209548, -0.37077501,\n",
       "        -0.54204166,  0.40015333,  0.29457433,  0.30822928,  0.70403166,\n",
       "        -1.60760001, -0.93711665,  0.63221   ,  0.59509267, -0.38296766,\n",
       "         2.95400006,  0.14456332, -0.09737834, -0.003224  , -0.06279167,\n",
       "         0.050218  ,  0.14238633,  0.131965  ,  0.14213333, -0.493325  ,\n",
       "        -0.159679  ,  0.1110655 , -0.28615333,  0.25916317,  0.1393735 ,\n",
       "         0.07512983,  0.0305865 , -0.33857917,  0.189849  ,  0.43066502]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\n",
    "    ['my', 'dog', 'is', 'very', 'happy'],\n",
    "    ['my', 'cat', 'is', 'instead', 'very', 'sad'],\n",
    "]\n",
    "\n",
    "w2v_features_test = w2v_transformer.fit_transform(text)\n",
    "w2v_features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMON model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of the SIMON method is that given a domain lexicon, the input text is measured against it, computing a vector that encodes the similarity between the input text and the lexicon. Such a vector encodes the similarity, as given by the word embedding model, of each of the words of the analyzed text to the lexicon words. For more information, please check [the documentation section](https://gsi-upm.github.io/gsitk/features/#simon) and the [original publication](https://doi.org/10.1016/j.knosys.2018.12.005)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using SIMON, first, you need to use a [word embedding](https://en.wikipedia.org/wiki/Word_embedding) model.\n",
    "The [gensim library](https://radimrehurek.com/gensim/index.html) includes some downloadable models, that can be accessed as shown:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the embedding model uses a lexicon of the domain to analyze; in this case, Sentiment Analysis. We can use the [Bing Liu lexicon](https://dl.acm.org/citation.cfm?id=1014073), accessible from NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "lexicon = [list(opinion_lexicon.positive()), list(opinion_lexicon.negative())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to configure the SIMON feature extractor. You can do this like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsitk.features import simon\n",
    "\n",
    "simon_transformer = simon.Simon(lexicon=lexicon, n_lexicon_words=50, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can extract features using the simon model. The implementation has also support for scikit-learn's Pipelines. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22.64666   ,  5.5705223 ,  2.2850964 , 10.810273  , 13.582376  ,\n",
       "        15.125259  ,  8.218765  ,  9.00664   , -0.6937979 ,  3.9982183 ,\n",
       "         6.4370413 ,  9.553367  , -0.5632133 , 13.165003  , 14.947266  ,\n",
       "        12.059111  , 12.084166  , 16.94381   , 14.256762  ,  9.00737   ,\n",
       "        13.533313  , 12.647817  ,  6.6219416 , 10.682739  ,  6.4929757 ,\n",
       "        15.598402  ,  6.787737  ,  9.797682  ,  8.292459  , 12.26892   ,\n",
       "        12.590726  ,  7.692454  ,  7.1737566 ,  2.7645953 , 10.165642  ,\n",
       "         8.804068  ,  8.408757  ,  6.1955237 ,  2.0015996 ,  2.2309947 ,\n",
       "         1.2787244 ,  4.7324104 , -3.376121  , 13.01586   , 15.482267  ,\n",
       "        18.318289  , 11.544971  ,  5.715289  ,  1.1589067 ,  2.513846  ,\n",
       "         2.0361245 ,  7.6383333 ,  3.632421  , -1.3370285 , -3.1686149 ,\n",
       "        10.457821  ,  8.004176  ,  9.27165   ,  0.26848498, 13.7307205 ,\n",
       "         0.66586924,  3.258816  , 13.448561  ,  6.2063417 ,  7.4360647 ,\n",
       "        13.86675   , 11.382271  ,  8.852619  , 10.839699  ,  4.0306945 ,\n",
       "         3.5801606 ,  5.652316  ,  7.7164745 ,  0.18364441,  0.35577607,\n",
       "         8.836479  , 10.631449  ,  6.5578055 ,  4.9766135 ,  7.304039  ,\n",
       "         1.4824042 ,  6.420419  , -0.29708278, 11.140987  , 11.104248  ,\n",
       "        14.517608  , 14.589582  ,  2.9442854 ],\n",
       "       [22.64666   ,  5.5705223 ,  5.0429745 , 10.810273  , 13.582375  ,\n",
       "        15.125259  ,  8.218765  ,  9.006639  , -0.1227181 ,  3.9982183 ,\n",
       "         6.4370413 ,  9.553365  , -0.43892786, 13.165003  , 14.947266  ,\n",
       "        12.059112  , 12.084166  , 16.94381   , 14.256763  ,  9.00737   ,\n",
       "        13.533313  , 12.647815  ,  6.6219416 , 10.682737  ,  6.4929757 ,\n",
       "        15.598401  ,  6.8988657 ,  9.797681  ,  8.292459  , 12.26892   ,\n",
       "        12.590726  ,  7.692454  ,  7.1737566 ,  5.890463  , 10.281912  ,\n",
       "         7.938674  ,  8.35267   ,  5.9062653 ,  2.0015996 ,  2.2309947 ,\n",
       "         1.2787243 ,  4.7324104 , -0.23842043, 13.015857  , 15.482265  ,\n",
       "        25.534483  , 11.544971  ,  7.4026384 ,  4.7278094 ,  2.8091795 ,\n",
       "         6.284559  ,  8.126308  ,  4.070155  ,  0.0385592 , -2.4845915 ,\n",
       "        10.457822  ,  8.004177  ,  9.27165   , -0.4482143 , 13.730719  ,\n",
       "         1.4019574 ,  5.206396  , 13.44856   ,  8.668956  ,  7.4360642 ,\n",
       "        13.86675   , 11.382271  ,  8.85262   , 10.839697  ,  7.406282  ,\n",
       "         3.5801613 ,  5.652316  ,  6.559163  , -0.39882052,  2.5723372 ,\n",
       "         8.836479  , 10.631447  ,  7.5776615 ,  5.649705  ,  8.961614  ,\n",
       "         2.4668307 ,  6.4204206 ,  1.1060963 , 11.383308  , 11.104248  ,\n",
       "        14.517608  , 14.589582  ,  6.9521146 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\n",
    "    ['my', 'dog', 'is', 'very', 'happy'],\n",
    "    ['my', 'cat', 'is', 'instead', 'very', 'sad'],\n",
    "]\n",
    "\n",
    "simon_features_test = simon_transformer.fit_transform(text)\n",
    "simon_features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_gsitk_ allows to save features to disk, just using one line of code.\n",
    "This is useful if the feature extraction process takes too long, and it is not practical to repeat it.\n",
    "Thus, you can just save the features to disk, to use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsitk.features import features\n",
    "\n",
    "features.save_features(simon_features_test, 'simon_features_test') # you need to give the features an unique name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the features are saved in disk, under the `$DATA_PATH/features` directory. In our example, in here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simon_features_test.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load them from disk, we use the same name as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_feats = features.load_features('simon_features_test')\n",
    "(my_feats == simon_features_test).all() # check if they are the same features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_gsitk_ includes functionalities to predict sentiment directly from text.\n",
    "In this line of work, one of the most common approaches in Sentiment Analysis is to use a sentiment lexicon -that directly encodes subjective sentiment information- by matching the lexicon's word to those of the analyzed texts.\n",
    "_gsitk_ implements this approach in the `LexiconSum` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsitk.classifiers import LexiconSum\n",
    "\n",
    "# use the existing Bing Liu's lexicon, \n",
    "bingliu_pos = {word: 1 for word in opinion_lexicon.positive()}\n",
    "bingliu_neg = {word: -1 for word in opinion_lexicon.negative()}\n",
    "bingliu_pos.update(bingliu_neg)\n",
    "\n",
    "ls = LexiconSum(bingliu_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\n",
    "    ['my', 'dog', 'is', 'a', 'good', 'and', 'happy', 'pet'],\n",
    "    ['my', 'cat', 'is', 'not', 'sad', 'just', 'mildly', 'bad'],\n",
    "    ['today' , 'i', 'am', 'sad'],\n",
    "]\n",
    "\n",
    "ls.predict(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation greatly eases the early stages of development, allowing users to quickly develop prototypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, _gsitk_ has useful utilities that allow you to easily configure a Sentiment Analysis evaluation.\n",
    "In this example, we show a demo on how to do that. For more information on evaluation using _gsitk_, please read the [documentation](https://gsi-upm.github.io/gsitk/evaluation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in all evaluation methodologies, we need some datasets from which we can evaluate our models.\n",
    "We have already loaded two datasets, so let's use one of them: the _IMDB_ dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fold</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4677</td>\n",
       "      <td>train</td>\n",
       "      <td>[i, understand, this, film, to, be, a, debut, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7632</td>\n",
       "      <td>train</td>\n",
       "      <td>[getting, to, work, on, this, film, when, it, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1181</td>\n",
       "      <td>train</td>\n",
       "      <td>[rachel, griffiths, writes, and, directs, this...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5050</td>\n",
       "      <td>train</td>\n",
       "      <td>[we, really, enjoyed, grey, owl, :, a, simple,...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>832</td>\n",
       "      <td>train</td>\n",
       "      <td>[interesting, how, much, more, realistic, bros...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id   fold                                               text polarity  \\\n",
       "0  4677  train  [i, understand, this, film, to, be, a, debut, ...        1   \n",
       "1  7632  train  [getting, to, work, on, this, film, when, it, ...        1   \n",
       "2  1181  train  [rachel, griffiths, writes, and, directs, this...        1   \n",
       "3  5050  train  [we, really, enjoyed, grey, owl, :, a, simple,...        1   \n",
       "4   832  train  [interesting, how, much, more, realistic, bros...        1   \n",
       "\n",
       "  rating  \n",
       "0      9  \n",
       "1     10  \n",
       "2      9  \n",
       "3      7  \n",
       "4      8  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['imdb'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to declare some feature extraction methods, as we want to compare them. In this example, we compare SIMON to an straight-forward 1-gram method. We prepare the 1-gram method below. Also, we prepare a full pipeline, including the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsitk.preprocess import JoinTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipelineA = Pipeline([\n",
    "    ('join', JoinTransformer()), # needed to use CountVectorizer\n",
    "    ('vect', CountVectorizer(max_features=50)),\n",
    "    ('scale', StandardScaler(with_mean=False)),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "pipelineA.name = '1-gram'\n",
    "\n",
    "simon_transformer = simon.Simon(lexicon=lexicon, n_lexicon_words=50, embedding=embedding_model)\n",
    "pipelineB = Pipeline([\n",
    "    ('simon', simon_transformer),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', LogisticRegression(solver='liblinear')),\n",
    "])\n",
    "pipelineB.name = 'simon'\n",
    "\n",
    "w2v_transformer = Word2VecFeatures(model=embedding_model)\n",
    "pipelineC = Pipeline([\n",
    "    ('w2v', w2v_transformer),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', LogisticRegression(solver='liblinear')),\n",
    "])\n",
    "pipelineC.name = 'w2v'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train our two methods using the dataset. We select the `train` fold from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "pipelineA.fit(\n",
    "    data['imdb'][data['imdb']['fold'] == 'train']['text'],\n",
    "    data['imdb'][data['imdb']['fold'] == 'train']['polarity'].values.astype(int),\n",
    ")\n",
    "\n",
    "pipelineB.fit(\n",
    "    data['imdb'][data['imdb']['fold'] == 'train']['text'],\n",
    "    data['imdb'][data['imdb']['fold'] == 'train']['polarity'].values.astype(int),\n",
    ")\n",
    "\n",
    "pipelineC.fit(\n",
    "    data['imdb'][data['imdb']['fold'] == 'train']['text'],\n",
    "    data['imdb'][data['imdb']['fold'] == 'train']['polarity'].values.astype(int),\n",
    ")\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation is performed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Features</th>\n",
       "      <th>Model</th>\n",
       "      <th>CV</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imdb</td>\n",
       "      <td>None</td>\n",
       "      <td>1-gram__imdb</td>\n",
       "      <td>False</td>\n",
       "      <td>0.64408</td>\n",
       "      <td>0.644085</td>\n",
       "      <td>0.64408</td>\n",
       "      <td>0.644077</td>\n",
       "      <td>0.64408</td>\n",
       "      <td>0.644077</td>\n",
       "      <td>join --&gt; vect --&gt; scale --&gt; clf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>imdb</td>\n",
       "      <td>None</td>\n",
       "      <td>simon__imdb</td>\n",
       "      <td>False</td>\n",
       "      <td>0.74892</td>\n",
       "      <td>0.749377</td>\n",
       "      <td>0.74892</td>\n",
       "      <td>0.748805</td>\n",
       "      <td>0.74892</td>\n",
       "      <td>0.748805</td>\n",
       "      <td>simon --&gt; scale --&gt; clf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>imdb</td>\n",
       "      <td>None</td>\n",
       "      <td>w2v__imdb</td>\n",
       "      <td>False</td>\n",
       "      <td>0.75284</td>\n",
       "      <td>0.752973</td>\n",
       "      <td>0.75284</td>\n",
       "      <td>0.752807</td>\n",
       "      <td>0.75284</td>\n",
       "      <td>0.752807</td>\n",
       "      <td>w2v --&gt; scale --&gt; clf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset Features         Model     CV accuracy precision_macro recall_macro  \\\n",
       "0    imdb     None  1-gram__imdb  False  0.64408        0.644085      0.64408   \n",
       "1    imdb     None   simon__imdb  False  0.74892        0.749377      0.74892   \n",
       "2    imdb     None     w2v__imdb  False  0.75284        0.752973      0.75284   \n",
       "\n",
       "  f1_weighted f1_micro  f1_macro                      Description  \n",
       "0    0.644077  0.64408  0.644077  join --> vect --> scale --> clf  \n",
       "1    0.748805  0.74892  0.748805          simon --> scale --> clf  \n",
       "2    0.752807  0.75284  0.752807            w2v --> scale --> clf  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gsitk.evaluation.evaluation import Evaluation\n",
    "\n",
    "# define datasets for evaluation: select test fold\n",
    "datasets_evaluation = {'imdb': data['imdb'][data['imdb']['fold'] == 'test']}\n",
    "\n",
    "# configure evaluation\n",
    "ev = Evaluation(tuples=None,\n",
    "                datasets=datasets_evaluation,\n",
    "                pipelines=[pipelineA, pipelineB, pipelineC])\n",
    "\n",
    "# perform evaluation, this can take a little long\n",
    "ev.evaluate()\n",
    "\n",
    "# results are stored in ev, and are in pandas DataFrame format\n",
    "ev.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, we have performed a full evaluation on our data, comparing the two approaches.\n",
    "It can be seen the details of the results table and how the names of the methods are formed.\n",
    "Please note that the word embedding model and hyperparameters of the rest of methods are not normally used in a real evaluation, but are just set for the example.\n",
    "The same situation occurs for the metrics obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial has shown how to use the main functionalities of _gsitk_. For more information, please check the [documentation](https://gsi-upm.github.io/gsitk/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
